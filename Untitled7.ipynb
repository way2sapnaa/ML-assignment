{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vWMg5TVj9T1p"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "\n",
        "Regression & Its Evaluation | Assignment\n",
        "Question 1: What is Simple Linear Regression?\n",
        "Simple Linear Regression is a method to predict the value of one variable (the dependent variable, or response) based on the value of one other variable (the independent variable, or predictor) by fitting a straight line through the data points.\n",
        "\n",
        "The Equation:\n",
        "The relationship is modeled by the equation of a straight line:\n",
        "\n",
        "\n",
        "Where:\n",
        "\n",
        " = Dependent variable (what you’re trying to predict)\n",
        " = Independent variable (the input/predictor)\n",
        " = Intercept (the value of Y when X = 0)\n",
        " = Slope (the change in Y for a one-unit change in X)\n",
        " = Error term (captures the difference between the actual and predicted values)\n",
        "Key Idea\n",
        "The goal is to find the line of best fit, which minimizes the total squared differences between the actual values and the predicted values. This is called the least squares method.\n",
        "\n",
        "Example\n",
        "Predicting someone’s weight (Y) based on their height (X).\n",
        "Predicting house price (Y) based on its size (X).\n",
        "Assumptions\n",
        "To use simple linear regression properly, you assume:\n",
        "\n",
        "The relationship between X and Y is linear.\n",
        "The residuals (errors) are normally distributed.\n",
        "Homoscedasticity — constant variance of residuals.\n",
        "Independence of observations.\n",
        "Question 2: What are the key assumptions of Simple Linear Regression?\n",
        "1. Linearity\n",
        "What it means: The relationship between the independent variable\n",
        " and the dependent variable\n",
        " is linear — that is, the change in\n",
        " is proportional to the change in\n",
        ".\n",
        "How to check: Look at a scatter plot of\n",
        " vs.\n",
        ". The points should form roughly a straight-line pattern.\n",
        "2️. Independence of Errors\n",
        "What it means: The residuals (errors) are independent — there should be no correlation between consecutive residuals.\n",
        "Why it matters: If errors are correlated (common in time series data), the standard errors may be underestimated.\n",
        "How to check: Use the Durbin-Watson test or plot residuals vs. time/order.\n",
        "3️. Homoscedasticity (Constant Variance of Errors)\n",
        "What it means: The variance of residuals should be constant across all levels of\n",
        ".\n",
        "Why it matters: If the spread of residuals grows or shrinks with\n",
        ", predictions become less reliable.\n",
        "How to check: Plot residuals vs. fitted values. The spread should look random and even — not a funnel shape.\n",
        "4️. Normality of Errors\n",
        "What it means: The residuals (errors) should be normally distributed around the regression line.\n",
        "Why it matters: This mainly affects confidence intervals and hypothesis tests.\n",
        "How to check: Use a histogram or a Q-Q plot of residuals.\n",
        "5️. No Significant Outliers or High Leverage Points\n",
        "What it means: Outliers can overly influence the slope and intercept.\n",
        "How to check: Look at scatter plots, Cook’s distance, or leverage plots.\n",
        "Summary of Assumptions:\n",
        "\n",
        "Assumption\tChecks for\n",
        "Linearity\tIs the relationship straight-line?\n",
        "Independence\tAre residuals independent?\n",
        "Homoscedasticity\tIs variance constant?\n",
        "Normality of Errors\tAre residuals normal?\n",
        "No major outliers\tAre there extreme values distorting the line?\n",
        "Question 3: What is heteroscedasticity, and why is it important to address in regression models?\n",
        "Heteroscedasticity means that the variance of the residuals (errors) is not constant across all levels of the independent variable(s).\n",
        "\n",
        "In simple words:\n",
        "\n",
        "With homoscedasticity, the spread of the residuals is even — the scatter around the regression line stays roughly the same for all predicted values.\n",
        "With heteroscedasticity, the spread changes — often getting wider or narrower as the value of X changes.\n",
        "Example\n",
        "Imagine you’re predicting people’s income based on their years of education.\n",
        "\n",
        "For people with low years of education, the variance in income is small (most earn similar amounts).\n",
        "For people with higher education, incomes can vary widely (some earn a lot, some not so much). This creates a fan shape in a residual plot — classic heteroscedasticity.\n",
        "How to Detect Heteroscedasticity\n",
        "Residuals vs. Fitted plot: If you see a funnel or cone shape (residuals spread out more as fitted values increase), it indicates heteroscedasticity.\n",
        "\n",
        "Formal tests:\n",
        "\n",
        "Breusch–Pagan test\n",
        "White’s test\n",
        "Why is Heteroscedasticity a Problem?\n",
        "Heteroscedasticity doesn’t bias the regression coefficients, but it violates an important OLS assumption, leading to:\n",
        "\n",
        "1️. Incorrect standard errors → t-tests & p-values become unreliable.\n",
        "\n",
        "2️. Confidence intervals & hypothesis tests may be invalid.\n",
        "\n",
        "3️. Model predictions may be less efficient (less precise).\n",
        "\n",
        "How to Fix It\n",
        "Possible solutions:\n",
        "\n",
        "Transform the dependent variable (e.g., log(Y), square root).\n",
        "Use Weighted Least Squares (WLS) instead of OLS.\n",
        "Use robust standard errors to adjust for the unequal variance.\n",
        "Sometimes add missing variables — heteroscedasticity can appear if a relevant variable is omitted.\n",
        "Question 4: What is Multiple Linear Regression?\n",
        "Multiple Linear Regression (MLR) is an extension of Simple Linear Regression — but instead of using one independent variable (predictor), it uses two or more.\n",
        "\n",
        "It models the relationship between:\n",
        "\n",
        "One dependent variable (Y)\n",
        "And two or more independent variables (X₁, X₂, X₃, …, Xₙ)\n",
        "The Equation\n",
        "The general form is:\n",
        "\n",
        "\n",
        "Where:\n",
        "\n",
        " = Dependent variable (what you’re predicting)\n",
        " = Independent variables (predictors)\n",
        " = Intercept\n",
        " = Regression coefficients (how much Y changes for a unit change in X, holding other X’s constant)\n",
        " = Error term (residual)\n",
        "What Does It Do?\n",
        "Multiple Linear Regression lets you:\n",
        "\n",
        "Predict Y using several factors at once.\n",
        "Understand the effect of each predictor while controlling for others.\n",
        "Estimate how much each X uniquely contributes to Y.\n",
        "Example\n",
        "Predicting house prices:\n",
        "\n",
        "\n",
        "Size → bigger houses cost more\n",
        "More rooms → higher price\n",
        "Closer to city → higher price\n",
        "Each coefficient shows the effect of one factor while holding the others constant.\n",
        "\n",
        "Key Assumptions\n",
        "Same as Simple Linear Regression plus:\n",
        "\n",
        "Linearity: Relationship between each X and Y is linear.\n",
        "Independence of errors: Residuals are independent.\n",
        "Homoscedasticity: Constant variance of residuals.\n",
        "Normality of errors: Residuals are normally distributed.\n",
        "No multicollinearity: Independent variables shouldn’t be highly correlated with each other. (High multicollinearity makes it hard to estimate individual effects.)\n",
        "Why Use Multiple Linear Regression?\n",
        "Real-world problems are rarely explained by one factor.\n",
        "MLR allows for better predictions.\n",
        "It helps understand the relative importance of each predictor.\n",
        "But: It’s more complex → more room for assumption violations.\n",
        "Question 5: What is polynomial regression, and how does it differ from linear regression?\n",
        "Polynomial Regression is an extension of Linear Regression that models the relationship between the independent variable(s) and the dependent variable as an nth-degree polynomial.\n",
        "\n",
        "In other words, instead of fitting a straight line, you fit a curved line to capture non-linear relationships.\n",
        "\n",
        "The General Equation\n",
        "A polynomial regression with one predictor looks like:\n",
        "\n",
        "\n",
        "Where:\n",
        "\n",
        " = Dependent variable\n",
        " = Predictor variable raised to different powers\n",
        " = Coefficients\n",
        " = Error term\n",
        "How is it Different from Linear Regression?\n",
        "Aspect\tSimple/Multiple Linear Regression\tPolynomial Regression\n",
        "Relationship\tModels linear relationships (straight line or hyperplane)\tModels non-linear relationships (curve)\n",
        "Predictors\tUses original X variables only\tUses original X plus powers of X\n",
        "Equation form\tStraight-line equation\tPolynomial equation\n",
        "Example\n",
        "Important Point\n",
        "Even though the relationship is non-linear in X, polynomial regression is still linear in the coefficients! That’s why it can be solved using ordinary least squares (OLS) just like linear regression.\n",
        "\n",
        "When to Use Polynomial Regression\n",
        "When your data shows a curved trend that a straight line can’t capture. Example:\n",
        "\n",
        "Growth curves (population growth)\n",
        "Price vs. advertising spend\n",
        "Physics or engineering problems where relationships bend\n",
        "Watch Out For:\n",
        "Overfitting: Higher-degree polynomials can fit the training data perfectly but fail to generalize.\n",
        "Extrapolation: Predictions outside the observed range can become wildly inaccurate.\n",
        "Collinearity: Powers of X are often correlated → can inflate variance.\n",
        "Visual Example\n",
        "Linear: Fits a straight line: ![Linear line]\n",
        "Polynomial: Fits a curve: ![Curved line]\n",
        "(Imagine a U-shaped scatter plot: a line cuts through awkwardly, but a quadratic curve fits nicely.)\n",
        "\n",
        "Key Difference:\n",
        "Linear Regression → Best for straight-line trends. Polynomial Regression → Best when the trend is curved, but you still want to use linear regression methods by transforming X.\n",
        "\n",
        "Question 6: Implement a Python program to fit a Simple Linear Regression model to the following sample data:\n",
        "● X = [1, 2, 3, 4, 5]\n",
        "\n",
        "● Y = [2.1, 4.3, 6.1, 7.9, 10.2]\n",
        "\n",
        "Plot the regression line over the data points.\n",
        "\n",
        "(Include your Python code and output in the code box below.)\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "# Sample data\n",
        "X = np.array([1, 2, 3, 4, 5]).reshape(-1, 1)\n",
        "Y = np.array([2.1, 4.3, 6.1, 7.9, 10.2])\n",
        "\n",
        "# Create and fit the model\n",
        "model = LinearRegression()\n",
        "model.fit(X, Y)\n",
        "\n",
        "# Predictions\n",
        "Y_pred = model.predict(X)\n",
        "\n",
        "# Plot\n",
        "plt.scatter(X, Y, color='blue', label='Data Points')\n",
        "plt.plot(X, Y_pred, color='red', label='Regression Line')\n",
        "plt.xlabel('X')\n",
        "plt.ylabel('Y')\n",
        "plt.title('Simple Linear Regression Example')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# Coefficients\n",
        "model.intercept_, model.coef_[0]\n",
        "\n",
        "\n",
        "(np.float64(0.17999999999999794), np.float64(1.9800000000000004))\n",
        "Question 7: Fit a Multiple Linear Regression model on this sample data:\n",
        "● Area = [1200, 1500, 1800, 2000]\n",
        "\n",
        "● Rooms = [2, 3, 3, 4]\n",
        "\n",
        "● Price = [250000, 300000, 320000, 370000]\n",
        "\n",
        "Check for multicollinearity using VIF and report the results. (Include your Python code and output in the code box below.)\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "import statsmodels.api as sm\n",
        "\n",
        "# Sample data\n",
        "Area = np.array([1200, 1500, 1800, 2000])\n",
        "Rooms = np.array([2, 3, 3, 4])\n",
        "Price = np.array([250000, 300000, 320000, 370000])\n",
        "\n",
        "# Prepare the data\n",
        "X = pd.DataFrame({\n",
        "    'Area': Area,\n",
        "    'Rooms': Rooms\n",
        "})\n",
        "Y = Price\n",
        "\n",
        "# Fit Multiple Linear Regression\n",
        "model = LinearRegression()\n",
        "model.fit(X, Y)\n",
        "\n",
        "# Add constant for statsmodels VIF\n",
        "X_with_const = sm.add_constant(X)\n",
        "\n",
        "# Calculate VIF\n",
        "vif_data = pd.DataFrame()\n",
        "vif_data['feature'] = X_with_const.columns\n",
        "vif_data['VIF'] = [variance_inflation_factor(X_with_const.values, i)\n",
        "                    for i in range(X_with_const.shape[1])]\n",
        "\n",
        "# Results\n",
        "model.intercept_, model.coef_, vif_data\n",
        "\n",
        "(np.float64(103157.89473684214),\n",
        " array([   63.15789474, 34736.84210526]),\n",
        "   feature        VIF\n",
        " 0   const  34.210526\n",
        " 1    Area   7.736842\n",
        " 2   Rooms   7.736842)\n",
        "Question 8: Implement polynomial regression on the following data:\n",
        "● X = [1, 2, 3, 4, 5]\n",
        "\n",
        "● Y = [2.2, 4.8, 7.5, 11.2, 14.7]\n",
        "\n",
        "Fit a 2nd-degree polynomial and plot the resulting curve. (Include your Python code and output in the code box below.)\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "# Sample data\n",
        "X = np.array([1, 2, 3, 4, 5]).reshape(-1, 1)\n",
        "Y = np.array([2.2, 4.8, 7.5, 11.2, 14.7])\n",
        "\n",
        "# Transform to 2nd-degree polynomial features\n",
        "poly = PolynomialFeatures(degree=2)\n",
        "X_poly = poly.fit_transform(X)\n",
        "\n",
        "# Fit the polynomial regression model\n",
        "model = LinearRegression()\n",
        "model.fit(X_poly, Y)\n",
        "\n",
        "# Predictions for smooth curve\n",
        "X_range = np.linspace(1, 5, 100).reshape(-1, 1)\n",
        "X_range_poly = poly.transform(X_range)\n",
        "Y_pred = model.predict(X_range_poly)\n",
        "\n",
        "# Plot\n",
        "plt.scatter(X, Y, color='blue', label='Data Points')\n",
        "plt.plot(X_range, Y_pred, color='red', label='2nd-Degree Polynomial Fit')\n",
        "plt.xlabel('X')\n",
        "plt.ylabel('Y')\n",
        "plt.title('Polynomial Regression (Degree 2)')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# Coefficients\n",
        "model.intercept_, model.coef_\n",
        "\n",
        "\n",
        "(np.float64(0.06000000000000938), array([0.  , 1.94, 0.2 ]))\n",
        "Question 9: Create a residuals plot for a regression model trained on this data:\n",
        "● X = [10, 20, 30, 40, 50]\n",
        "\n",
        "● Y = [15, 35, 40, 50, 65]\n",
        "\n",
        "Assess heteroscedasticity by examining the spread of residuals. (Include your Python code and output in the code box below.)\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "# Sample data\n",
        "X = np.array([10, 20, 30, 40, 50]).reshape(-1, 1)\n",
        "Y = np.array([15, 35, 40, 50, 65])\n",
        "\n",
        "# Fit Linear Regression\n",
        "model = LinearRegression()\n",
        "model.fit(X, Y)\n",
        "\n",
        "# Predictions and residuals\n",
        "Y_pred = model.predict(X)\n",
        "residuals = Y - Y_pred\n",
        "\n",
        "# Plot residuals\n",
        "plt.scatter(Y_pred, residuals, color='blue')\n",
        "plt.axhline(y=0, color='red', linestyle='--')\n",
        "plt.xlabel('Predicted Values')\n",
        "plt.ylabel('Residuals')\n",
        "plt.title('Residuals Plot')\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# Residuals\n",
        "residuals\n",
        "\n",
        "\n",
        "array([-3. ,  5.5, -1. , -2.5,  1. ])\n",
        "Question 10: Imagine you are a data scientist working for a real estate company. You need to predict house prices using features like area, number of rooms, and location. However, you detect heteroscedasticity and multicollinearity in your regression model. Explain the steps you would take to address these issues and ensure a robust model.\n",
        "Goal: Predict house prices using features like area, number of rooms, and location.\n",
        "Challenges: Heteroscedasticity: The spread of residuals is not constant — larger houses may have wider price variability. Multicollinearity: Predictors like area and number of rooms are likely correlated.\n",
        "1️. How to Address Heteroscedasticity\n",
        "Why it’s a problem:\n",
        "\n",
        "Heteroscedasticity doesn’t bias coefficients, but it makes standard errors unreliable → bad p-values → misleading significance tests.\n",
        "Common solutions:\n",
        "\n",
        "1️. Transform the dependent variable (Y):\n",
        "\n",
        "Take the log of house prices. This often stabilizes variance.\n",
        "\n",
        "New model:\n",
        "\n",
        "\n",
        "2️. Use robust standard errors:\n",
        "\n",
        "Instead of changing the model, adjust the calculation of standard errors to be robust to heteroscedasticity.\n",
        "E.g., in Python: statsmodels → HC0, HC3 robust covariance estimators.\n",
        "3️. Weighted Least Squares (WLS):\n",
        "\n",
        "Give less weight to observations with higher variance.\n",
        "More complex, but effective for severe heteroscedasticity.\n",
        "4️. Check for omitted variables:\n",
        "\n",
        "Sometimes heteroscedasticity appears because important predictors are missing (e.g., neighborhood income level).\n",
        "2️. How to Address Multicollinearity\n",
        "Why it’s a problem:\n",
        "\n",
        "High correlation among predictors inflates standard errors → unstable estimates → hard to interpret individual effects.\n",
        "Common solutions:\n",
        "\n",
        "1️. Check VIF:\n",
        "\n",
        "Drop or combine predictors with high VIF (typically > 10 is a red flag).\n",
        "Example: If area and rooms are highly correlated, maybe replace them with a composite feature like area per room.\n",
        "2️. Use dimensionality reduction:\n",
        "\n",
        "Principal Component Analysis (PCA): Combines correlated variables into uncorrelated components.\n",
        "This sacrifices interpretability but can stabilize the model.\n",
        "3️. Regularization:\n",
        "\n",
        "Use Ridge Regression (L2 penalty) to shrink coefficients of correlated predictors.\n",
        "Or Lasso Regression (L1 penalty) to shrink some coefficients to zero → automatic feature selection.\n",
        "Example: sklearn.linear_model.Ridge or Lasso.\n",
        "4️. Domain knowledge:\n",
        "\n",
        "Maybe you realize area is more meaningful than rooms, so you drop rooms altogether.\n",
        "3️. Model Diagnostics and Validation\n",
        "After fixing issues:\n",
        "\n",
        "Re-check residual plots to confirm variance is more stable.\n",
        "Re-calculate VIF to verify multicollinearity is under control.\n",
        "Use cross-validation to check generalization.\n",
        "Compare model performance (R², RMSE) with and without fixes.\n",
        "4️. Final Deliverable\n",
        "Provide a clear, interpretable model:\n",
        "\n",
        "Communicate the impact of each predictor.\n",
        "Explain any transformations used (e.g., log price).\n",
        "Justify any features dropped or combined.\n"
      ]
    }
  ]
}